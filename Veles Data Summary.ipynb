{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a416d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.precision', 2)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fedc4",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d58b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_grid(linear_range=[0, 1, 2, 3], grid_shape=[2, 3]):\n",
    "    '''Return a list of 2-D coordinates corresponding to a each element of a 1-D array/list of objects, in shape = grid_shape\n",
    "    Useful for converting a linear index into a coordinate grid for charting purposes. Note that elements of\n",
    "    linear range that do not fit in a grid of shape = grid_shape will be truncated.\n",
    "    Arguments:\n",
    "    | -- linear_range, list of ints: input indexes to be converted to a 2-d grid.\n",
    "    | -- grid_shape, tuple of ints: the shape of the coordinate grid returned\n",
    "    Returns:\n",
    "        a dict with keys = indexes of the linear_range, and values = grid coordinates.\n",
    "    '''\n",
    "    # make a list of indices for the linear_range\n",
    "    linear_range = [_ for _ in dict(enumerate(linear_range))]\n",
    "    grid_coords = []\n",
    "    for r in range(grid_shape[0]):\n",
    "        for c in range(grid_shape[1]):\n",
    "            grid_coords.append((r, c))\n",
    "\n",
    "    return dict(zip(linear_range, grid_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77c7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickplot(data, plot_title='', ylabel='', xlabel='', legend=[], yaxisformat=\"{x:,.2f}\"):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "    ax.plot(data)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    ax.set_title(plot_title)\n",
    "    #     ax.xaxis.set_major_locator(tkr.MultipleLocator())\n",
    "    ax.xaxis.set_major_locator(tkr.MaxNLocator(10))\n",
    "    ax.yaxis.set_major_formatter(yaxisformat)\n",
    "    if len(legend)>0:\n",
    "        ax.legend(legend)\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b706c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_codes():\n",
    "    sc = {\n",
    "        'Andhra Pradesh': 'ANP',\n",
    "        'Arunachal Pradesh': 'ARP',\n",
    "        'Assam': 'ASS',\n",
    "        'Bihar': 'BIH',\n",
    "        'Chhattisgarh': 'CHH',\n",
    "        'Delhi': 'DEL',\n",
    "        'Gujarat': 'GUJ',\n",
    "        'Haryana': 'HAR',\n",
    "        'Himachal Pradesh': 'HIP',\n",
    "        'Jammu & Kashmir': 'JAK',\n",
    "        'Jharkhand': 'JHA',\n",
    "        'Karnataka': 'KAR',\n",
    "        'Kerala': 'KER',\n",
    "        'Madhya Pradesh': 'MAP',\n",
    "        'Maharashtra': 'MAH',\n",
    "        'Manipur': 'MAN',\n",
    "        'Meghalaya': 'MEG',\n",
    "        'Mizoram': 'MIZ',\n",
    "        'Nagaland': 'NAG',\n",
    "        'Odisha': 'ODI',\n",
    "        'Punjab': 'PUN',\n",
    "        'Rajasthan': 'RAJ',\n",
    "        'Sikkim': 'SIK',\n",
    "        'Tamil Nadu': 'TAM',\n",
    "        'Telangana': 'TEL',\n",
    "        'Tripura': 'TRI',\n",
    "        'Uttar Pradesh': 'UTP',\n",
    "        'Uttarakhand': 'UTT',\n",
    "        'West Bengal': 'WBE',\n",
    "        'Goa': 'GOA',\n",
    "        'Ladakh': 'LAD'\n",
    "    }\n",
    "    sc = {k.upper():v for k,v in sc.items()}\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47450dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_name(state_code:str):\n",
    "    # get the state codes and flip keys/values    \n",
    "    sc = {v:k for k,v in get_state_codes().items()}\n",
    "    # return the state name (upper case)\n",
    "    return sc[state_code].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7c7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_states(state_codes:dict):    \n",
    "    anon_root = 'State_'\n",
    "    anon_sc = {}\n",
    "    for i,(k,v) in enumerate(state_codes.items()):\n",
    "        anon_sc[k] = anon_root+str(i+1)\n",
    "        \n",
    "    return anon_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52186672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basin_codes():\n",
    "    '''Returns a dict of river basin names and 3-letter codes.'''\n",
    "    bc = {\n",
    "        'Brahmani and Baitarni': 'BAB',\n",
    "        'Barak and others': 'BAR',\n",
    "        'Brahamaputra': 'BRA',\n",
    "        'Cauvery': 'CAU',\n",
    "        'Ganga': 'GAN',\n",
    "        'Godavari': 'GOD',\n",
    "        'Indus (up to border)': 'IND',\n",
    "        'West flowing rivers of Kutch and Saurashtra including Luni': 'KAS',\n",
    "        'Krishna': 'KRI',\n",
    "        'Minor rivers draining into Myanmar and Bangladesh': 'MAB',\n",
    "        'Mahi': 'MAH',\n",
    "        'East flowing rivers between Mahanadi and Pennar': 'MAP',\n",
    "        'Narmada': 'NAR',\n",
    "        'Area of North Ladakh not draining into Indus basin': 'NLA',\n",
    "        'East flowing rivers between Pennar and Kanyakumari': 'PAK',\n",
    "        'Pennar': 'PEN',\n",
    "        'Area of inland drainage in Rajasthan': 'RAJ',\n",
    "        'Sabarmati': 'SAB',\n",
    "        'Subernarekha': 'SUB',\n",
    "        'Tapi': 'TAP',\n",
    "        'West flowing rivers from Tadri to Kanyakumari': 'TTK',\n",
    "        'West flowing rivers from Tapi to Tadri': 'TTT'\n",
    "        }   \n",
    "    \n",
    "    bc = {k.upper():v for k,v in bc.items()}\n",
    "    return bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979d11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basin_name(basin_code:str):\n",
    "    '''Returns the basin name for a passed basin code.'''\n",
    "    # get the basin codes and flip keys/values    \n",
    "    sc = {v:k for k,v in get_basin_codes().items()}\n",
    "    # return the basin name (upper case)\n",
    "    return sc[basin_code].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aeea4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_basins(basin_codes:dict):    \n",
    "    '''Takes the dictionary of basin codes and returns an anonymized dict with the same keys and new values.'''\n",
    "    anon_root = 'Basin_'\n",
    "    anon_bc = {}\n",
    "    for i,(k,v) in enumerate(basin_codes.items()):\n",
    "        anon_bc[k] = anon_root+str(i+1)\n",
    "        \n",
    "    return anon_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc0903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrological data\n",
    "def state_profile(state_code: str):\n",
    "    # each state has its own time series of actual, moving averages, and deviation\n",
    "    # get the hydrological data for each state\n",
    "    # rainfall\n",
    "    # gw\n",
    "    # reservoir\n",
    "    # ET (later)\n",
    "    # soil moisture (later)\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 3, figsize=(10,10))\n",
    "    plt.show()\n",
    "\n",
    "# price data\n",
    "# aggregate the price data \n",
    "\n",
    "# chart daily rainfall data for each basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683f6ef",
   "metadata": {},
   "source": [
    "#### Root folders / filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72737dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_root_folder = r'C:/Users/viren/Documents/___UChicago MSFM/water index/'\n",
    "save_file_actual = f'{_root_folder}/_consolidated data/actual_rf_all_states.xlsx'\n",
    "save_file_normal = f'{_root_folder}/_consolidated data/normal_rf_all_states.xlsx'\n",
    "\n",
    "# reservoir levels\n",
    "res_level_save_file = _root_folder+r'_consolidated data/reservoir_levels_all_states.xlsx'\n",
    "res_storage_save_file = _root_folder+r'_consolidated data/reservoir_storage_all_states.xlsx'\n",
    "\n",
    "# groundwater levels\n",
    "gw_level_save_file = _root_folder+r'_consolidated data/groundwater_levels_all_states.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1556c",
   "metadata": {},
   "source": [
    "#### Cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093c56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_typos(names):\n",
    "    typos = {\"TAMILNADU\": \"TAMIL NADU\"}\n",
    "    repl = list(map(lambda x: x.upper(), names))\n",
    "    for i, _ in enumerate(repl):\n",
    "        if _.upper() in typos:\n",
    "            repl[i] = typos[_].upper()\n",
    "    return repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495b3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_states(df: pd.DataFrame):\n",
    "    matches = len(list(enumerate(map(lambda x: x.upper(),filter(lambda x: x.upper() in get_state_codes(), df.index)))))\n",
    "    print(f\"State match found for: {matches} states.\")\n",
    "\n",
    "    print(\"State match not found for:\")\n",
    "    list(enumerate(map(lambda x: x.upper(),filter(lambda x: x.upper() not in get_state_codes(), df.index))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed68a36",
   "metadata": {},
   "source": [
    "### Read+Save: Daily Rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5c1816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_rf_data(input_year: int):\n",
    "    # read data for input year\n",
    "    raw_rainfall_all_states = pd.read_excel(f'{_root_folder}/rainfall data daily - state/rainfall_State Wise {input_year}.xls')\n",
    "    raw_rainfall_all_states.ffill(axis=1, inplace=True)\n",
    "    raw_rainfall_all_states.set_axis(\n",
    "        raw_rainfall_all_states.iloc[0,:]+\" \"+raw_rainfall_all_states.iloc[1,:],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "    raw_rainfall_all_states = raw_rainfall_all_states.iloc[2:,:]\n",
    "    raw_rainfall_all_states.set_index(np.nan, inplace=True)\n",
    "    raw_rainfall_all_states.rename_axis(\"Rainfall\",axis=0,inplace=True)\n",
    "    # drop cumulative columns\n",
    "    raw_rainfall_all_states = raw_rainfall_all_states.loc[\n",
    "        :,[\"Cumulative\" not in _ for _ in raw_rainfall_all_states.columns]\n",
    "    ]\n",
    "\n",
    "    # standardize index\n",
    "    # fix typos in index\n",
    "    raw_rainfall_all_states.index = fix_typos(raw_rainfall_all_states.index) \n",
    "    # filter for states in the main list\n",
    "    states_found = list(filter(lambda x: x in get_state_codes(), raw_rainfall_all_states.index))\n",
    "    states_not_found = list(filter(lambda x: x not in get_state_codes(), raw_rainfall_all_states.index))\n",
    "    print(f'\\n| Input year: {input_year}') \n",
    "    print(f'| States found in data: {len(states_found)}') \n",
    "    print(f'| States not found in data ({len(states_not_found)}): {states_not_found}') \n",
    "    raw_rainfall_all_states = raw_rainfall_all_states.loc[states_found,:]\n",
    "    \n",
    "    # replace '-' with np.nan\n",
    "    raw_rainfall_all_states = raw_rainfall_all_states.replace(to_replace='-', value=0.0)\n",
    "    \n",
    "    print(raw_rainfall_all_states.iloc[:10,[0,-1]])\n",
    "    return raw_rainfall_all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd21e8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def consolidate_actual_rainfall(raw_rainfall_all_states):\n",
    "    try:\n",
    "        actual_rf_all_states = pd.read_excel(save_file_actual, index_col=0)\n",
    "        print(f'Actual rainfall data - all states - found >>\\n')\n",
    "        print(actual_rf_all_states.iloc[[0,-1],:5])\n",
    "        # concatenate the new actual data \n",
    "        new_actual_rf_all_states = raw_rainfall_all_states.filter(regex=r'Actual')\n",
    "        # rename column axis and trim text\n",
    "        new_actual_rf_all_states.rename_axis(\"Actual (mm)\", axis=1, inplace=True)\n",
    "        new_actual_rf_all_states.columns = list(\n",
    "            map(lambda x: pd.to_datetime(x.replace(\" Actual (mm)\",\"\")),\n",
    "                new_actual_rf_all_states.columns)\n",
    "        )\n",
    "        new_actual_rf_all_states = new_actual_rf_all_states.T\n",
    "        print('Concatenating new data:\\n')\n",
    "        actual_rf_all_states = pd.concat((actual_rf_all_states, new_actual_rf_all_states), sort=True)  \n",
    "        print(actual_rf_all_states.iloc[[0,-1],:5])\n",
    "        actual_rf_all_states.to_excel(save_file_actual)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # create the dataframe\n",
    "        actual_rf_all_states = raw_rainfall_all_states.filter(regex=r'Actual')\n",
    "        # rename column axis and trim text\n",
    "        actual_rf_all_states.rename_axis(\"Actual (mm)\", axis=1, inplace=True)\n",
    "        actual_rf_all_states.columns = list(\n",
    "            map(lambda x: pd.to_datetime(x.replace(\" Actual (mm)\",\"\")),\n",
    "                actual_rf_all_states.columns)\n",
    "        )\n",
    "        actual_rf_all_states = actual_rf_all_states.T\n",
    "        # actual_rf_all_states = actual_rf_all_states.reset_index().melt(id_vars=['index']).set_index(['index', 'variable']\n",
    "        actual_rf_all_states.to_excel(save_file_actual)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08a5dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def consolidate_normal_rainfall(raw_rainfall_all_states):\n",
    "    try:\n",
    "        normal_rf_all_states = pd.read_excel(save_file_normal, index_col=0)\n",
    "        print(f'Normal rainfall data - all states - found >>\\n')\n",
    "        print(normal_rf_all_states.iloc[[0,-1],:5])\n",
    "            # concatenate the new normal data \n",
    "        new_normal_rf_all_states = raw_rainfall_all_states.filter(regex=r'Normal')\n",
    "        # rename column axis and trim text\n",
    "        new_normal_rf_all_states.rename_axis(\"Normal (mm)\", axis=1, inplace=True)\n",
    "        new_normal_rf_all_states.columns = list(\n",
    "            map(lambda x: pd.to_datetime(x.replace(\" Normal (mm)\",\"\")),\n",
    "                new_normal_rf_all_states.columns)\n",
    "        )\n",
    "        new_normal_rf_all_states = new_normal_rf_all_states.T\n",
    "        print('Concatenating new data:\\n')\n",
    "        normal_rf_all_states = pd.concat((normal_rf_all_states, new_normal_rf_all_states), sort=True)  \n",
    "        print(normal_rf_all_states.iloc[[0,-1],:5])\n",
    "        normal_rf_all_states.to_excel(save_file_normal)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        normal_rf_all_states = raw_rainfall_all_states.filter(regex=r'Normal')\n",
    "        # rename column axis and trim text\n",
    "        normal_rf_all_states.rename_axis(\"Normal (mm)\", axis=1, inplace=True)\n",
    "        normal_rf_all_states.columns = list(\n",
    "            map(lambda x: pd.to_datetime(x.replace(\" Normal (mm)\",\"\")),\n",
    "                normal_rf_all_states.columns)\n",
    "        )\n",
    "        normal_rf_all_states = normal_rf_all_states.T\n",
    "        # normal_rf_all_states = normal_rf_all_states.reset_index().melt(id_vars=['index']).set_index(['index', 'variable'])\n",
    "        normal_rf_all_states.to_excel(save_file_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40d0fb",
   "metadata": {},
   "source": [
    "### Read+Save: Reservoir Levels & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb11a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_resvr_data(input_year: int):\n",
    "    # read data for input year\n",
    "    raw_resvr_all_states = pd.read_excel(f'{_root_folder}/reservoir data daily - state/reservoir_Level & Storage {input_year}.xls')\n",
    "    raw_resvr_all_states\n",
    "\n",
    "    raw_resvr_all_states.iloc[:,0].fillna(\"\", inplace=True)\n",
    "    raw_resvr_all_states.iloc[:,0] = fix_typos(raw_resvr_all_states.iloc[:,0]) \n",
    "\n",
    "    # filter for states in the main list\n",
    "    states_found = list(filter(lambda x: x in get_state_codes(), raw_resvr_all_states.iloc[:,0]))\n",
    "    states_not_found = [_ for _ in get_state_codes() if _ not in states_found]\n",
    "    print(f'| Reading reservoir data...')\n",
    "    print(f'\\n| Input year: {input_year}')\n",
    "    print(f'| States found in data: {len(states_found)}') \n",
    "    print(f'| States not found in data ({len(states_not_found)}): {states_not_found}')\n",
    "\n",
    "    # raw_resvr_all_states = raw_resvr_all_states.loc[states_found,:]\n",
    "\n",
    "    # extract a column of just states\n",
    "    raw_resvr_all_states = raw_resvr_all_states.assign(State=lambda x: [_ if _ in states_found else np.nan for _ in x.iloc[:,0]])\n",
    "\n",
    "\n",
    "    raw_resvr_all_states = raw_resvr_all_states[\n",
    "        [raw_resvr_all_states.columns[-1]]+[_ for _ in raw_resvr_all_states.columns[:-1]]\n",
    "    ]\n",
    "\n",
    "    raw_resvr_all_states.iloc[:,0].ffill(inplace=True)\n",
    "    raw_resvr_all_states.iloc[0,:].ffill(inplace=True)\n",
    "    raw_resvr_all_states.set_axis(raw_resvr_all_states.iloc[0,:]+\" \"+raw_resvr_all_states.iloc[1,:],\n",
    "                                  axis=1, inplace=True)\n",
    "    raw_resvr_all_states = raw_resvr_all_states.iloc[2:,:]\n",
    "    raw_resvr_all_states.replace(to_replace=\"-\", value=np.nan, inplace=True)\n",
    "    \n",
    "    resvr_totals = pd.DataFrame()\n",
    "\n",
    "    # sum up levels and storage by state\n",
    "    _res_states =list(filter(lambda x: x is not np.nan, raw_resvr_all_states.iloc[:,0].unique()))\n",
    "    _res_states    \n",
    "\n",
    "    for _state in _res_states:\n",
    "        state_reservoirs = raw_resvr_all_states.loc[raw_resvr_all_states.iloc[:,0] == _state,:].fillna(0.0)\n",
    "        resvr_totals[_state] = state_reservoirs.sum(axis=0)[2:]\n",
    "\n",
    "    resvr_totals.fillna(0.0, inplace=True)\n",
    "\n",
    "    print(resvr_totals.iloc[:10,[0,-1]])\n",
    "    return resvr_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76768303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_resvr_levels_storage(resvr_totals: pd.DataFrame):\n",
    "    \n",
    "    # Save reservoir levels\n",
    "    reservoir_levels = resvr_totals.filter(regex=r'Level', axis=0)\n",
    "    reservoir_levels.index = pd.DatetimeIndex(list(map(lambda x: x.replace(r\"Level(m)\",\"\"), reservoir_levels.index)))\n",
    "    reservoir_levels.rename_axis(\"Resvr Level (m)\", axis=0, inplace=True)\n",
    "        \n",
    "    # try concatenating new data to existing data\n",
    "    try:\n",
    "        existing_data = pd.read_excel(res_level_save_file, index_col=0)\n",
    "        print('\\nConcatenating new reservoir level data >>')\n",
    "        reservoir_levels = pd.concat((existing_data, reservoir_levels), sort=True)\n",
    "        print(reservoir_levels.iloc[[0,-1],:5])\n",
    "        print('Saving:', res_level_save_file)\n",
    "        reservoir_levels.to_excel(res_level_save_file)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print('Saving:', res_level_save_file)\n",
    "        reservoir_levels.to_excel(res_level_save_file)\n",
    "    \n",
    "    # Save reservoir storage     \n",
    "    reservoir_storage = resvr_totals.filter(regex=r'Storage', axis=0)\n",
    "    reservoir_storage.index = pd.DatetimeIndex(list(map(lambda x: x.replace(r\"Storage(BCM)\",\"\"), reservoir_storage.index)))\n",
    "    reservoir_storage.rename_axis(\"Resvr Storage (BCM)\", axis=0, inplace=True)\n",
    "\n",
    "    try:\n",
    "        existing_data = pd.read_excel(res_storage_save_file, index_col=0)\n",
    "        print('\\nConcatenating new reservoir storage data >>')\n",
    "        reservoir_storage = pd.concat((existing_data, reservoir_storage), sort=True)\n",
    "        print(reservoir_storage.iloc[[0,-1],:5])\n",
    "        print('Saving:', res_level_save_file)\n",
    "        reservoir_storage.to_excel(res_storage_save_file)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print('Saving:', res_storage_save_file)\n",
    "        reservoir_storage.to_excel(res_storage_save_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e34e7d",
   "metadata": {},
   "source": [
    "### Read+Save: Groundwater Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a10843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_gw_data(input_year: int):\n",
    "    # read data for input year\n",
    "    raw_gw_all_states = pd.read_excel(f'{_root_folder}/groundwater data - state - all agencies/groundwater_State wise Level {input_year}.xls')\n",
    "    raw_gw_all_states\n",
    "\n",
    "    raw_gw_all_states.iloc[:,0].fillna(\"\", inplace=True)\n",
    "    raw_gw_all_states.iloc[:,0] = fix_typos(raw_gw_all_states.iloc[:,0]) \n",
    "\n",
    "    # filter for states in the main list\n",
    "    states_found = list(filter(lambda x: x in get_state_codes(), raw_gw_all_states.iloc[:,0]))\n",
    "    states_not_found = [_ for _ in get_state_codes() if _ not in states_found]\n",
    "    print(f'| Reading groundwater data...')\n",
    "    print(f'\\n| Input year: {input_year}')\n",
    "    print(f'| States found in data: {len(states_found)}') \n",
    "    print(f'| States not found in data ({len(states_not_found)}): {states_not_found}')\n",
    "    raw_gw_all_states.set_axis(raw_gw_all_states.iloc[0,:], axis=1, inplace=True)\n",
    "\n",
    "    raw_gw_all_states = raw_gw_all_states.iloc[2:,:]\n",
    "\n",
    "    raw_gw_all_states.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    raw_gw_all_states.set_index(raw_gw_all_states.iloc[:,0], inplace=True)\n",
    "\n",
    "    raw_gw_all_states.rename_axis(\"GW Level (mbgl)\", axis=0, inplace=True)\n",
    "\n",
    "    raw_gw_all_states = raw_gw_all_states.iloc[:,1:]\n",
    "\n",
    "    raw_gw_all_states = raw_gw_all_states.T\n",
    "\n",
    "    raw_gw_all_states.index = pd.DatetimeIndex(raw_gw_all_states.index)\n",
    "    \n",
    "    gw_all_states = raw_gw_all_states.replace(to_replace=\"-\", value=np.nan)\n",
    "    \n",
    "    return gw_all_states  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b451cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gw_levels(groundwater_levels: pd.DataFrame):\n",
    "        \n",
    "    # try concatenating new data to existing data\n",
    "    try:\n",
    "        existing_data = pd.read_excel(gw_level_save_file, index_col=0)\n",
    "        print('\\nConcatenating new groundwater level data >>')\n",
    "        groundwater_levels = pd.concat((existing_data, groundwater_levels), sort=True)\n",
    "        print(groundwater_levels.iloc[[0,-1],:5])\n",
    "        print('Saving:', gw_level_save_file)\n",
    "        groundwater_levels.to_excel(gw_level_save_file)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print('Saving:', gw_level_save_file)\n",
    "        groundwater_levels.to_excel(gw_level_save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaa2a3",
   "metadata": {},
   "source": [
    "### Update Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76563c8",
   "metadata": {},
   "source": [
    "#### Rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73bd3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_year = 1901\n",
    "start_year = 1986\n",
    "end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "090d4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read data\n",
    "# for yr in range(start_year, end_year+1):\n",
    "#     raw_rainfall_all_states = read_rf_data(input_year=yr)\n",
    "#     consolidate_actual_rainfall(raw_rainfall_all_states)\n",
    "#     consolidate_normal_rainfall(raw_rainfall_all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847e70c",
   "metadata": {},
   "source": [
    "#### Reservoir Levels and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4acf7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_year = 1999\n",
    "# end_year = 2022\n",
    "\n",
    "# for yr in range(start_year, end_year+1):\n",
    "#     resvr_totals = read_resvr_data(input_year = yr)\n",
    "#     save_resvr_levels_storage(resvr_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e9d68",
   "metadata": {},
   "source": [
    "#### Groundwater Levels (mbgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a21faff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_year = 1994\n",
    "# end_year = 2022\n",
    "\n",
    "# for yr in range(start_year, end_year+1):\n",
    "#     groundwater_levels = read_gw_data(input_year=yr)\n",
    "#     save_gw_levels(groundwater_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6acf81",
   "metadata": {},
   "source": [
    "### State Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c32a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ce3af7760f12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xx' is not defined"
     ]
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9c5f3",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "anonymize = False\n",
    "\n",
    "# pick a date range\n",
    "start_date = '1/1/1999'\n",
    "end_date = ''\n",
    "\n",
    "\n",
    "# rainfall dataset\n",
    "dataset = 'actual'\n",
    "if dataset == 'actual':\n",
    "    save_file = save_file_actual\n",
    "else:\n",
    "    save_file = save_file_normal\n",
    "# consolidated data\n",
    "imd_grid_rainfall = pd.read_excel(save_file, index_col=0)\n",
    "\n",
    "\n",
    "# reservoir data\n",
    "dataset = 'storage'\n",
    "\n",
    "# consolidated data\n",
    "if dataset == 'storage':\n",
    "    reservoir_data = pd.read_excel(res_storage_save_file, index_col=0)\n",
    "    _chart_title = \"Storage (BCM)\"\n",
    "else:\n",
    "    reservoir_data = pd.read_excel(res_level_save_file, index_col=0)\n",
    "    _chart_title = \"Level (m)\"\n",
    "    \n",
    "\n",
    "# groundwater dataset\n",
    "dataset = 'all_agencies'\n",
    "\n",
    "# consolidated data\n",
    "if dataset == 'all_agencies':\n",
    "    groundwater_data = pd.read_excel(\n",
    "        f'{_root_folder}_consolidated data/groundwater_levels_all_states.xlsx', index_col=0)\n",
    "    groundwater_data = groundwater_data.ffill(axis=0)\n",
    "else:    \n",
    "    groundwater_data = pd.read_excel(\n",
    "        f'{_root_folder}_consolidated data/groundwater_levels_all_states_CGWB.xlsx', index_col=0)\n",
    "    groundwater_data = groundwater_data.ffill(axis=0)\n",
    "\n",
    "    \n",
    "# roll through the various states\n",
    "for test_state in range(len(get_state_codes())):\n",
    "    input_state_name = [k for k in get_state_codes()][test_state]\n",
    "    print(f'>> State selected: {input_state_name}')\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # ST MAVG days\n",
    "    _d = 365\n",
    "\n",
    "    # LT MAVG years\n",
    "    _y = 5\n",
    "\n",
    "    # state profile chart\n",
    "    fig, ax = plt.subplots(3,3, figsize = (15, 10))\n",
    "    if anonymize:\n",
    "            _state_name = anonymize_states(get_state_codes())[input_state_name]\n",
    "    else:\n",
    "            _state_name = input_state_name\n",
    "    try:\n",
    "        fig.suptitle(f'STATE: {_state_name} | {start_date:%d-%b-%Y} to {end_date:%d-%b-%Y}', y=1.00, fontsize=13)\n",
    "    except ValueError:\n",
    "        fig.suptitle(f'STATE: {_state_name} | {start_date:%d-%b-%Y} to latest avail.', y=1.00, fontsize=13)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------RAINFALL------------------------------------------------------- #\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    # filter consolidated data\n",
    "    filtered_data = pd.DataFrame(imd_grid_rainfall.loc[start_date:end_date, input_state_name])\n",
    "    filtered_data \n",
    "\n",
    "    as_of_date = max(filtered_data.index)\n",
    "    print(f'>> Rainfall data as of: {as_of_date: %d-%b-%Y}')\n",
    "\n",
    "    ax[0][0].set_title(\"Daily Rainfall (mm)\")\n",
    "    ax[0][0].grid()\n",
    "    ax[0][0].plot(filtered_data)\n",
    "\n",
    "    st_mavg = pd.DataFrame()\n",
    "    st_mavg[f'{_d}_day_mavg'] = filtered_data.rolling(_d).mean()\n",
    "    ax[1][0].set_title(\"Short-Term MAVG Daily Rainfall (mm)\")\n",
    "    ax[1][0].grid()\n",
    "    ax[1][0].plot(st_mavg)\n",
    "    ax[1][0].legend(st_mavg.columns)\n",
    "\n",
    "    lt_mavg = pd.DataFrame()\n",
    "    lt_mavg[f'{_y}_yr_mavg'] = filtered_data.rolling(_y*365).mean()\n",
    "    ax[2][0].set_title(\"Long-Term MAVG Daily Rainfall (mm)\")\n",
    "    ax[2][0].grid()\n",
    "    ax[2][0].plot(lt_mavg)\n",
    "    ax[2][0].legend(lt_mavg.columns)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "    # ----------------------------------------RESERVOIR LEVELS------------------------------------------------- #\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    # filter consolidated data\n",
    "    try:\n",
    "        filtered_data = pd.DataFrame(reservoir_data.loc[start_date:end_date, input_state_name])\n",
    "    except KeyError:\n",
    "        filtered_data = pd.DataFrame(columns=[input_state_name], index=reservoir_data.index)\n",
    "\n",
    "    as_of_date = max(filtered_data.index)\n",
    "    print(f'>> Reservoir data as of: {as_of_date: %d-%b-%Y}')\n",
    "\n",
    "    ax[0][1].set_title(f\"Daily Reservoir {_chart_title}\")\n",
    "    ax[0][1].grid()\n",
    "    ax[0][1].plot(filtered_data, color='darkred')\n",
    "\n",
    "    st_mavg = pd.DataFrame()\n",
    "    # _d = 365\n",
    "    st_mavg[f'{_d}_day_mavg'] = filtered_data.rolling(_d).mean()\n",
    "\n",
    "    ax[1][1].set_title(f\"Short-Term MAVG Daily Resvr {_chart_title}\")\n",
    "    ax[1][1].grid()\n",
    "    ax[1][1].plot(st_mavg, color='darkred')\n",
    "    ax[1][1].legend(st_mavg.columns)\n",
    "\n",
    "    lt_mavg = pd.DataFrame()\n",
    "    # _y = 10\n",
    "    lt_mavg[f'{_y}_yr_mavg'] = filtered_data.rolling(_y*365).mean()\n",
    "    ax[2][1].set_title(f\"Long-Term MAVG Daily Resvr {_chart_title}\")\n",
    "    ax[2][1].grid()\n",
    "    ax[2][1].plot(lt_mavg, color='darkred')\n",
    "    ax[2][1].legend(lt_mavg.columns)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "    # -----------------------------------------GROUNDWATER LEVELS---------------------------------------------- #\n",
    "    # --------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    # filter consolidated data\n",
    "    filtered_data = pd.DataFrame(groundwater_data.loc[start_date:end_date, input_state_name])\n",
    "    # negate data for charts\n",
    "    filtered_data = -filtered_data\n",
    "\n",
    "    as_of_date = max(filtered_data.index)\n",
    "    print(f'>> Groundwater level data as of: {as_of_date: %d-%b-%Y}')\n",
    "\n",
    "    ax[0][2].set_title(\"Groundwater Level (mbgl)\")\n",
    "    ax[0][2].grid()\n",
    "    ax[0][2].plot(filtered_data, color='g')\n",
    "\n",
    "    st_mavg = pd.DataFrame()\n",
    "    st_mavg[f'{_d}_day_mavg'] = filtered_data.rolling(_d).mean()\n",
    "    ax[1][2].set_title(\"Short-Term MAVG Groundwater Level (mbgl)\")\n",
    "    ax[1][2].grid()\n",
    "    ax[1][2].plot(st_mavg, color='g')\n",
    "    ax[1][2].legend(st_mavg.columns)\n",
    "\n",
    "    lt_mavg = pd.DataFrame()\n",
    "    lt_mavg[f'{_y}_yr_mavg'] = filtered_data.rolling(_y*365).mean()\n",
    "    ax[2][2].set_title(\"Long-Term MAVG Groundwater Level (mbgl)\")\n",
    "    ax[2][2].grid()\n",
    "    ax[2][2].plot(lt_mavg, color='g')\n",
    "    ax[2][2].legend(lt_mavg.columns)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    _profile_filename = f'StateProfile_v2_{_state_name}.png'\n",
    "    plt.savefig(f'{_root_folder}_consolidated data/state_profiles/{_profile_filename}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e4fe8",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee7e5e",
   "metadata": {},
   "source": [
    "### Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12325b",
   "metadata": {},
   "source": [
    "# read in data\n",
    "xlfile = pd.ExcelFile('./pricing/India Water Price Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002210b1",
   "metadata": {},
   "source": [
    "xlfile.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15b03f",
   "metadata": {},
   "source": [
    "cities = ['Delhi', 'Chennai','Hyderabad','Bengaluru','Mumbai']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2f132",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# consolidated data \n",
    "cons_prices = pd.DataFrame()\n",
    "\n",
    "for city in cities:\n",
    "    cons_prices = pd.concat((cons_prices,xlfile.parse(city)), ignore_index=True)\n",
    "    print(cons_prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0301473",
   "metadata": {},
   "source": [
    "cons_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce64c9",
   "metadata": {},
   "source": [
    "# save consolidated prices to consolidated data folder\n",
    "savefile_prices = _root_folder+'_consolidated data/consolidated_prices.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587cd5c",
   "metadata": {},
   "source": [
    "# save prices to excel\n",
    "cons_prices.to_excel(savefile_prices, sheet_name='ConsolidatedPrices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f056ae",
   "metadata": {},
   "source": [
    "# read consolidated prices \n",
    "price_data = pd.read_excel(savefile_prices, index_col=0)\n",
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf7551",
   "metadata": {},
   "source": [
    "# def urban_price_plot(city: str, consolidated_price_data: pd.DataFrame)\n",
    "\n",
    "price_data = pd.read_excel(savefile_prices, index_col=0)\n",
    "print(price_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965bf46",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "city = 'Mumbai'\n",
    "\n",
    "# filter for city\n",
    "filtered_data = price_data[price_data.City==city]\n",
    "price_types = filtered_data['Price Type'].unique()\n",
    "print(f'Price types: {price_types}')\n",
    "filtered_data\n",
    "\n",
    "# for each price type, make a bar chart\n",
    "fig_cols = 3\n",
    "fig_rows = int(np.ceil(len(price_types)/fig_cols))\n",
    "\n",
    "print(f'fig_rows: {fig_rows}, fig_cols: {fig_cols}')\n",
    "fig, ax = plt.subplots(fig_rows,\n",
    "                       fig_cols,\n",
    "                       figsize=(20,10*fig_rows),\n",
    "                       sharey=True,\n",
    "                       sharex=False\n",
    "                      )\n",
    "\n",
    "ltg = line_to_grid(price_types, grid_shape=[fig_rows, fig_cols])\n",
    "print(ltg)\n",
    "\n",
    "for i,pt in enumerate(price_types):\n",
    "    # get the data for this price type\n",
    "    chart_data = filtered_data[filtered_data['Price Type'] == pt][\n",
    "        ['Monthly Consumption (kL)', 'Volumetric Rate - USD/acre-ft']\n",
    "    ].set_index('Monthly Consumption (kL)')\n",
    "    print(chart_data)\n",
    "    legend = ''\n",
    "    ylabel = 'Volumetric Rate - USD/acre-ft'\n",
    "    xlabel = 'Monthly Consumption (kL)'\n",
    "    yaxisformat = '{x:,.2f}'\n",
    "\n",
    "    plot_title = city+' | '+pt\n",
    "    _x = ltg[i][0]\n",
    "    _y = ltg[i][1]\n",
    "    print(_x, _y)\n",
    "    \n",
    "    if fig_rows > 1 and fig_cols > 1:\n",
    "        ax[_x][_y].bar(x=chart_data.index, height=chart_data.values.squeeze(), align='center')\n",
    "        ax[_x][_y].set_title(plot_title)\n",
    "        ax[_x][_y].set_xticks(chart_data.index)\n",
    "        ax[_x][_y].set_xticklabels(chart_data.index, rotation=45)\n",
    "        ax[_x][_y].yaxis.set_major_formatter(yaxisformat)\n",
    "        if len(legend)>0:\n",
    "            ax[_x][_y].legend(legend)\n",
    "        ax[_x][_y].set_ylabel(ylabel)\n",
    "        ax[_x][_y].set_xlabel(xlabel)\n",
    "\n",
    "        ax[_x][_y].grid()\n",
    "    else:\n",
    "        ax[_y].bar(x=chart_data.index, height=chart_data.values.squeeze(), align='center')\n",
    "        ax[_y].set_title(plot_title)\n",
    "        if city != 'Bengaluru':\n",
    "            ax[_y].set_xticks(chart_data.index)\n",
    "            ax[_y].set_xticklabels(chart_data.index, rotation=45)\n",
    "        if city == 'Mumbai':\n",
    "            ax[_y].yaxis.set_major_locator(tkr.LogLocator(base=5))\n",
    "            ax[_y].set_xticks(chart_data.index)\n",
    "            ax[_y].set_xticklabels(chart_data.index, rotation=45)\n",
    "        ax[_y].yaxis.set_major_formatter(yaxisformat)\n",
    "        if len(legend)>0:\n",
    "            ax[_y].legend(legend)\n",
    "        ax[_y].set_ylabel(ylabel)\n",
    "        ax[_y].set_xlabel(xlabel)\n",
    "        ax[_y].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d463f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "num = 1\n",
    "\n",
    "save_folder = 'C:/Users/viren/Documents/___UChicago MSFM/water index/__Data sources/SLUSI'\n",
    "# for num in range(1,1837):\n",
    "URL = f\"http://slusi.dacnet.nic.in/dss/dssabstracts/dss_{num}.pdf\"\n",
    "urllib.(URL,\n",
    "                   f\"{save_folder}/dss_1.pdf\")\n",
    "\n",
    "\n",
    "# html_page = urlopen(req)\n",
    "\n",
    "# soup = BeautifulSoup(html_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1b6db",
   "metadata": {},
   "source": [
    "## Soil and Land Use Surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebd0cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "def download_file(download_url, save_folder, filename):\n",
    "    urlretrieve(download_url, f'{save_folder}/{filename}')\n",
    "\n",
    "save_folder = 'C:/Users/viren/Documents/___UChicago MSFM/water index/__Data sources/SLUSI'\n",
    "not_found = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae4b1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dss_1707 saved.\n",
      "dss_1708 saved.\n",
      "HTTP Error 404: Not Found: dss_1709 not found.\n",
      "HTTP Error 404: Not Found: dss_1710 not found.\n",
      "HTTP Error 404: Not Found: dss_1711 not found.\n",
      "dss_1712 saved.\n",
      "dss_1713 saved.\n",
      "dss_1714 saved.\n",
      "dss_1715 saved.\n",
      "dss_1716 saved.\n",
      "dss_1717 saved.\n",
      "dss_1718 saved.\n",
      "dss_1719 saved.\n",
      "dss_1720 saved.\n",
      "dss_1721 saved.\n",
      "dss_1722 saved.\n",
      "dss_1723 saved.\n",
      "dss_1724 saved.\n",
      "dss_1725 saved.\n",
      "dss_1726 saved.\n",
      "dss_1727 saved.\n",
      "dss_1728 saved.\n",
      "dss_1729 saved.\n",
      "dss_1730 saved.\n",
      "dss_1731 saved.\n",
      "dss_1732 saved.\n",
      "dss_1733 saved.\n",
      "dss_1734 saved.\n",
      "dss_1735 saved.\n",
      "dss_1736 saved.\n",
      "dss_1737 saved.\n",
      "dss_1738 saved.\n",
      "dss_1739 saved.\n",
      "dss_1740 saved.\n",
      "dss_1741 saved.\n",
      "dss_1742 saved.\n",
      "dss_1743 saved.\n",
      "dss_1744 saved.\n",
      "dss_1745 saved.\n",
      "dss_1746 saved.\n",
      "dss_1747 saved.\n",
      "dss_1748 saved.\n",
      "dss_1749 saved.\n",
      "dss_1750 saved.\n",
      "dss_1751 saved.\n",
      "dss_1752 saved.\n",
      "dss_1753 saved.\n",
      "dss_1754 saved.\n",
      "dss_1755 saved.\n",
      "dss_1756 saved.\n",
      "dss_1757 saved.\n",
      "dss_1758 saved.\n",
      "dss_1759 saved.\n",
      "dss_1760 saved.\n",
      "dss_1761 saved.\n",
      "HTTP Error 404: Not Found: dss_1762 not found.\n",
      "dss_1763 saved.\n",
      "dss_1764 saved.\n",
      "dss_1765 saved.\n",
      "dss_1766 saved.\n",
      "dss_1767 saved.\n",
      "dss_1768 saved.\n",
      "dss_1769 saved.\n",
      "dss_1770 saved.\n",
      "dss_1771 saved.\n",
      "dss_1772 saved.\n",
      "dss_1773 saved.\n",
      "dss_1774 saved.\n",
      "dss_1775 saved.\n",
      "dss_1776 saved.\n",
      "dss_1777 saved.\n",
      "dss_1778 saved.\n",
      "dss_1779 saved.\n",
      "dss_1780 saved.\n",
      "dss_1781 saved.\n",
      "dss_1782 saved.\n",
      "dss_1783 saved.\n",
      "dss_1784 saved.\n",
      "dss_1785 saved.\n",
      "dss_1786 saved.\n",
      "dss_1787 saved.\n",
      "dss_1788 saved.\n",
      "dss_1789 saved.\n",
      "dss_1790 saved.\n",
      "dss_1791 saved.\n",
      "dss_1792 saved.\n",
      "dss_1793 saved.\n",
      "dss_1794 saved.\n",
      "dss_1795 saved.\n",
      "dss_1796 saved.\n",
      "HTTP Error 404: Not Found: dss_1797 not found.\n",
      "dss_1798 saved.\n",
      "dss_1799 saved.\n",
      "dss_1800 saved.\n",
      "HTTP Error 404: Not Found: dss_1801 not found.\n",
      "dss_1802 saved.\n",
      "HTTP Error 404: Not Found: dss_1803 not found.\n",
      "dss_1804 saved.\n",
      "HTTP Error 404: Not Found: dss_1805 not found.\n",
      "HTTP Error 404: Not Found: dss_1806 not found.\n",
      "HTTP Error 404: Not Found: dss_1807 not found.\n",
      "HTTP Error 404: Not Found: dss_1808 not found.\n",
      "HTTP Error 404: Not Found: dss_1809 not found.\n",
      "HTTP Error 404: Not Found: dss_1810 not found.\n",
      "HTTP Error 404: Not Found: dss_1811 not found.\n",
      "dss_1812 saved.\n",
      "HTTP Error 404: Not Found: dss_1813 not found.\n",
      "dss_1814 saved.\n",
      "dss_1815 saved.\n",
      "dss_1816 saved.\n",
      "dss_1817 saved.\n",
      "dss_1818 saved.\n",
      "HTTP Error 404: Not Found: dss_1819 not found.\n",
      "HTTP Error 404: Not Found: dss_1820 not found.\n",
      "dss_1821 saved.\n",
      "dss_1822 saved.\n",
      "dss_1823 saved.\n",
      "dss_1824 saved.\n",
      "dss_1825 saved.\n",
      "HTTP Error 404: Not Found: dss_1826 not found.\n",
      "HTTP Error 404: Not Found: dss_1827 not found.\n",
      "HTTP Error 404: Not Found: dss_1828 not found.\n",
      "HTTP Error 404: Not Found: dss_1829 not found.\n",
      "dss_1830 saved.\n",
      "HTTP Error 404: Not Found: dss_1831 not found.\n",
      "HTTP Error 404: Not Found: dss_1832 not found.\n",
      "HTTP Error 404: Not Found: dss_1833 not found.\n",
      "dss_1834 saved.\n",
      "HTTP Error 404: Not Found: dss_1835 not found.\n",
      "dss_1836 saved.\n"
     ]
    }
   ],
   "source": [
    "for num in range(1707,1837):\n",
    "    if num >= 1707:\n",
    "        URL = f\"https://slusi.dacnet.nic.in/dss/dssabstracts/dss_{num}.pdf\"\n",
    "    else:\n",
    "        URL = f\"http://slusi.dacnet.nic.in/dss/dssabstracts/dss_{num}.pdf\"\n",
    "    \n",
    "    try:\n",
    "        download_file(URL, save_folder, f'dss_{num}.pdf')\n",
    "        print(f'dss_{num} saved.')\n",
    "    except Exception as e:\n",
    "        not_found.append(f'dss_{num}')\n",
    "        print(f'{e}: dss_{num} not found.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6141b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd80590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2074e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb94310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37165831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6563d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65130eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eadb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43335bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc89f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
